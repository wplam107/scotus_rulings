{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Justice Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:26:37.096756Z",
     "start_time": "2020-07-16T22:26:36.989873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Import data\n",
    "import pickle\n",
    "\n",
    "# Scotus class object\n",
    "from scotus_class import scotus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T21:58:34.453957Z",
     "start_time": "2020-07-16T21:58:34.447206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Binary\n",
    "f = open('adj_df.p', 'rb')\n",
    "df1 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Multiclass\n",
    "f = open('mul_df.p', 'rb')\n",
    "df2 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test of Predicting Justice Opinions\n",
    "In this initial test, each case a justice has produced an opinion on is a potential target variable.  Justice Rehnquist being the first justice to retire on the list of justices evaluated is the test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:27:06.409074Z",
     "start_time": "2020-07-16T22:27:06.389653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate scotus object for references\n",
    "sc_obj = scotus(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:31:01.622986Z",
     "start_time": "2020-07-16T22:31:01.618185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Court Number(s): [0]\n"
     ]
    }
   ],
   "source": [
    "# Select Rehnquist\n",
    "sc_obj.justice_courts('Rehnquist')\n",
    "print('Court Number(s):', sc_obj.j_courts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:31:26.801249Z",
     "start_time": "2020-07-16T22:31:26.796731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Breyer', 'Ginsburg', 'Kennedy', \"O'Connor\", 'Rehnquist', 'Scalia', 'Souter', 'Stevens', 'Thomas']\n"
     ]
    }
   ],
   "source": [
    "# Rehnquist court\n",
    "rehn_court = sc_obj.courts[0]\n",
    "print(rehn_court)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:47:09.343006Z",
     "start_time": "2020-07-16T22:47:09.337941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Case: 0\n",
      "Last Case: 497\n"
     ]
    }
   ],
   "source": [
    "# Rehnquist term\n",
    "print('First Case:', sc_obj.j_cases['Rehnquist'][0])\n",
    "print('Last Case:', sc_obj.j_cases['Rehnquist'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:43:22.047219Z",
     "start_time": "2020-07-16T22:43:22.039951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing justice opinions: 36\n"
     ]
    }
   ],
   "source": [
    "current_df = df1.loc[rehn_court, :498]\n",
    "print('Missing justice opinions:', current_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:45:32.771069Z",
     "start_time": "2020-07-16T22:45:32.763822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases with no missing opinions: 465\n"
     ]
    }
   ],
   "source": [
    "# Drop cases with missing opinions\n",
    "current_df.dropna(axis=1, inplace=True)\n",
    "print('Cases with no missing opinions:', len(current_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T23:07:47.932585Z",
     "start_time": "2020-07-16T23:07:47.923783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set X, y helper function\n",
    "def get_xy(df, justice, case):\n",
    "    X = df.drop(justice, axis=0).drop(case, axis=1)\n",
    "    y = df.drop(justice, axis=0)[case]\n",
    "    return X, y\n",
    "    \n",
    "# Prediction function\n",
    "def predict_cases(df, justice):\n",
    "    cases = list(df.columns)\n",
    "    preds = []\n",
    "    for case in cases:\n",
    "        clf = BernoulliNB()\n",
    "        X, y = get_xy(df, justice, case)\n",
    "        clf.fit(X, y)\n",
    "        pred = int(clf.predict(np.array(df.loc[justice].drop(case)).reshape(1, len(cases)-1)))\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T23:47:01.929926Z",
     "start_time": "2020-07-16T23:46:43.386819Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justice: Breyer\n",
      "------------------------------\n",
      "MSE: 0.13548387096774195\n",
      "F1-Score: 0.9142857142857143\n",
      "Accuracy: 0.864516129032258\n",
      "------------------------------\n",
      "Justice: Ginsburg\n",
      "------------------------------\n",
      "MSE: 0.09032258064516129\n",
      "F1-Score: 0.9418282548476455\n",
      "Accuracy: 0.9096774193548387\n",
      "------------------------------\n",
      "Justice: Kennedy\n",
      "------------------------------\n",
      "MSE: 0.13118279569892474\n",
      "F1-Score: 0.9242236024844721\n",
      "Accuracy: 0.8688172043010752\n",
      "------------------------------\n",
      "Justice: O'Connor\n",
      "------------------------------\n",
      "MSE: 0.15698924731182795\n",
      "F1-Score: 0.9129916567342073\n",
      "Accuracy: 0.843010752688172\n",
      "------------------------------\n",
      "Justice: Rehnquist\n",
      "------------------------------\n",
      "MSE: 0.12043010752688173\n",
      "F1-Score: 0.9261213720316623\n",
      "Accuracy: 0.8795698924731182\n",
      "------------------------------\n",
      "Justice: Scalia\n",
      "------------------------------\n",
      "MSE: 0.10752688172043011\n",
      "F1-Score: 0.9307479224376731\n",
      "Accuracy: 0.8924731182795699\n",
      "------------------------------\n",
      "Justice: Souter\n",
      "------------------------------\n",
      "MSE: 0.10752688172043011\n",
      "F1-Score: 0.9320652173913043\n",
      "Accuracy: 0.8924731182795699\n",
      "------------------------------\n",
      "Justice: Stevens\n",
      "------------------------------\n",
      "MSE: 0.13978494623655913\n",
      "F1-Score: 0.9048316251830161\n",
      "Accuracy: 0.8602150537634409\n",
      "------------------------------\n",
      "Justice: Thomas\n",
      "------------------------------\n",
      "MSE: 0.1032258064516129\n",
      "F1-Score: 0.9335180055401662\n",
      "Accuracy: 0.896774193548387\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for justice in rehn_court:\n",
    "    preds = list(predict_cases(current_df, justice))\n",
    "    real = list(current_df.loc[justice])\n",
    "    diff = pd.Series([ preds[i] - real[i] for i in range(len(preds)) ])\n",
    "    mse = sum(diff**2) / len(brey_real)\n",
    "    correct = diff.value_counts()[0]\n",
    "    false_neg = diff.value_counts()[-1] # Joined majority but predicted dissent\n",
    "    false_pos = diff.value_counts()[1] # Dissent but predicted join majority\n",
    "    true_pos = 0\n",
    "    for i in range(len(diff)):\n",
    "        if preds[i] == 1 and real[i] == 1:\n",
    "            true_pos += 1\n",
    "    true_neg = correct - true_pos\n",
    "    precision = true_pos / (true_pos+false_pos)\n",
    "    recall = true_pos / (true_pos+false_neg)\n",
    "    f1 = 2 * (precision*recall) / (precision+recall)\n",
    "    acc = correct / len(diff)\n",
    "    print('Justice:', justice)\n",
    "    print('-'*30)\n",
    "    print('MSE:', mse)\n",
    "    print('F1-Score:', f1)\n",
    "    print('Accuracy:', acc)\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T23:50:37.864658Z",
     "start_time": "2020-07-16T23:50:37.857483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     1.0\n",
       "6     0.0\n",
       "7     1.0\n",
       "8     1.0\n",
       "9     0.0\n",
       "10    1.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    0.0\n",
       "14    1.0\n",
       "15    1.0\n",
       "16    1.0\n",
       "17    1.0\n",
       "18    0.0\n",
       "19    1.0\n",
       "20    0.0\n",
       "21    1.0\n",
       "Name: Stevens, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_df.loc['Stevens'][:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T23:51:03.524405Z",
     "start_time": "2020-07-16T23:51:01.592182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cases(current_df, 'Stevens')[:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scotus",
   "language": "python",
   "name": "scotus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
